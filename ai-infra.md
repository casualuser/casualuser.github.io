---
layout: page
title: "AI & LLM Infrastructure"
permalink: /ai-infra/
---

# AI & LLM Infrastructure

I design and operate production-grade AI/LLM infrastructure that connects models, data, and applications with robust cloud and DevOps practices.

## Focus Areas

- Multi-provider LLM integrations (OpenAI-like, Anthropic, Hugging Face, custom models)
- Orchestration with LangChain (Python & JS), LangGraph, Crew.AI, Flowise, Hatchet
- Retrieval-Augmented Generation (RAG) pipelines with domain-specific data processing
- Evaluation and routing using benchmarks like SWE-bench and custom metrics
- Secure, observable deployments on AWS, Kubernetes, and serverless platforms

## Example Work

- **Merchant & chatbot platforms (Agentsmith)**  
  Integrated Hugging Face models, Oogabooga, LangChain, Flowise, and merchant-focused assistants into a unified system with Docker and CI/CD. Designed workflows for fine-tuning, evaluation, and safe rollout.

- **YC-backed AI startup (Hong Kong)**  
  Connected multiple AI tools and LLM providers to production infrastructure, enabling advanced debugging, experimentation, and gradual deployment of new capabilities.

- **US law firms association â€“ AI CRM**  
  Built AI-based CRM automation that analyzes emotional context in client responses, integrating AI calls into existing workflows with observability and safe failure modes.

- **Healthcare AI services**  
  Architected AI services in healthcare contexts with HIPAA, SOC 2, and GDPR readiness, emphasizing data protection, auditability, and robust monitoring.

## Typical Deliverables

- End-to-end AI/LLM pipelines from prompt to production, including data prep and retrieval
- Infrastructure-as-code (Terraform / Serverless Framework) for reproducible environments
- Kubernetes- and serverless-based deployment architectures with CI/CD
- Monitoring, logging, and tracing for model performance and reliability
- Documentation and onboarding materials for engineering and operations teams
